I want to run airflow on 5 nodes: master, node1, node2, node3, node4 
which are Rocky linux. I've made airflow user on each node. 
I want to use mysql as database for it.

create an airflow user on each node:
useradd -m airflow
passwd airflow


1. Install Required Packages on All Nodes
    sudo dnf update
    sudo dnf install -y python3 python3-devel python3-pip gcc gcc-c++ make mariadb mariadb-devel mariadb-connector-c mariadb-connector-c-devel
    export MYSQLCLIENT_CFLAGS=$(mariadb_config --cflags)
    export MYSQLCLIENT_LDFLAGS=$(mariadb_config --libs)
    pip3 install --no-cache-dir --force-reinstall mysqlclient
    pip3 install --upgrade pip setuptools wheel

2. Install Airflow on All Nodes
    export AIRFLOW_HOME=~/airflow
    pip3 install apache-airflow[celery,mysql]

3. Configure MySQL Database (Master Node) if it's not installed on any other node.
    Install MySQL:
        sudo dnf install -y mysql-server
        sudo systemctl enable --now mysqld
    Secure MySQL:
        mysql_secure_installation
    Create Airflow Database and User:
        mysql -u root -h host_ip -p
        CREATE DATABASE airflow_db;
        CREATE USER 'airflow'@'name_of_node_or_ip' IDENTIFIED BY 'any_password';
        SET PASSWORD FOR 'airflow'@'name_of_node_or_ip' = 'any_password';
        GRANT ALL PRIVILEGES ON *.* TO 'airflow'@'name_of_node_or_ip' WITH GRANT OPTION;
        FLUSH PRIVILEGES;
    Allow remote connections in /etc/my.cnf:
        [mysqld]
        bind-address=0.0.0.0
    Restart MySQL:
        sudo systemctl restart mysqld

4. Set Up Redis on Master Node (For Celery)
    Step 1: Redis installation:
        sudo dnf install -y redis
        sudo systemctl enable --now redis
        sudo systemctl start redis
    Step 2: Allow Remote Connections to Redis
        sudo nano /etc/redis/redis.conf
        change "bind 127.0.0.1" to "bind 0.0.0.0"
        change "protected-mode yes" to "protected-mode no"
        sudo systemctl restart redis
    Step 3: Open Firewall Ports (If Enabled)
        sudo firewall-cmd --permanent --add-port=6379/tcp
        sudo firewall-cmd --reload
    Step 5: Test Connection From Worker Nodes
        redis-cli -h master -p 6379 ping

5. Configure Airflow (On All Nodes)
    nano ~/airflow/airflow.cfg
    sql_alchemy_conn = mysql+pymysql://airflow:any_password@mysql_host_ip/airflow_db
    executor = CeleryExecutor
    broker_url = redis://master:6379/0
    result_backend = db+mysql://airflow:any_password@mysql_host_ip/airflow_db

6. Initialize and Start Airflow
    Run on the master node:
        airflow db init
        airflow users create --username admin --firstname Admin --lastname User --role Admin --email admin@example.com --password admin
    Run on all nodes:
        nohup airflow celery worker > ~/airflow/logs/celery_worker.log 2>&1 &
    Run on the master node:
        nohup airflow scheduler > ~/airflow/logs/scheduler.log 2>&1 &
        nohup airflow webserver -p 8081 > ~/airflow/logs/webserver.log 2>&1 &
        nohup airflow celery flower --broker=redis://master:6379/0 > ~/airflow/logs/flower.log 2>&1 &


**To stoping process:
    pkill -f "put_the_command_here"
    ps aux | grep "put_the_command_here"

**To test a dag:
    airflow dags trigger test_dag